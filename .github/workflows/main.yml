name: Build
on:
  push:
    branches:
      - '**'
  pull_request:
    branches:
      - '**'
  schedule:
    - cron: '0 8 * * *'

jobs:
  # First, determine which targets to run
  determine-targets:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - id: set-matrix
        run: |
          if [[ "${{ github.ref_name }}" =~ ^target/ ]]; then
            # Extract target name from branch (target/<target>/<feature>)
            TARGET=$(echo "${{ github.ref_name }}" | cut -d'/' -f2)
            # Convert to uppercase for comparison
            TARGET_UPPER=$(echo "$TARGET" | tr '[:lower:]' '[:upper:]')
            # Create matrix with only the matching target
            echo "matrix={\"target\":[\"$(echo $TARGET_UPPER)\"]}" >> $GITHUB_OUTPUT
          else
            # If not a target-specific branch, include all targets
            echo "matrix={\"target\":[\"XPS-9730\",\"R710\",\"R630\",\"Z390\"]}" >> $GITHUB_OUTPUT
          fi

  # Main build job that uses the determined matrix  
  build:
    needs: determine-targets
    runs-on: [self-hosted, Linux]
    timeout-minutes: 800
    strategy:
      matrix: ${{ fromJson(needs.determine-targets.outputs.matrix) }}
      fail-fast: false
    env:
      target: ${{ matrix.target }}
      stage4_fs: "./stage4"
    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Prepare Build Environment
      run: |
        apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends xz-utils python3-pip python3-venv rsync wget curl tree pipx
        pipx install awscli
        echo "/root/.local/bin" >> $GITHUB_PATH
        wget https://github.com/mikefarah/yq/releases/download/v4.40.5/yq_linux_amd64 -O /usr/local/bin/yq
        chmod +x /usr/local/bin/yq

    - name: Prepare stage3
      run: |
        set -x

        BASEURL="http://distfiles.gentoo.org/releases/amd64/autobuilds/"
        # Fetch the latest stage3 file name from the latest-stage3-amd64-openrc-splitusr.txt
        LATEST_INFO_URL="${BASEURL}latest-stage3-amd64-openrc-splitusr.txt"
        STAGE3=$(wget -q -O - ${LATEST_INFO_URL} | grep -oP '\d+T\d+Z/stage3-amd64-openrc-splitusr-\d+T\d+Z\.tar\.xz' | head -n 1)

        if [ -z "${STAGE3}" ]; then
            echo "Failed to find the latest stage3 filename."
            exit 1
        fi

        # Download the stage3 tarball
        mkdir -p /dl
        wget -c -O "/dl/${STAGE3##*/}" "${BASEURL}${STAGE3}"

        # Verify if the file has been successfully downloaded
        if [ -f "/dl/${STAGE3##*/}" ]; then
          mkdir -p ${{ env.stage4_fs }}
          tar -xJpf "/dl/${STAGE3##*/}" -C ${{ env.stage4_fs }} || (echo "Extraction failed. Removing downloaded file." && rm "/dl/${STAGE3##*/}")
        else
            echo "The download of the stage3 file failed."
            exit 1
        fi

    - name: Prepare configs
      run: |
        python3 -m venv ~/.venv
        ~/.venv/bin/pip install jinja2-cli

        # Path to the config.yml file
        CONFIG_FILE="config.yml"

        # Path to the Jinja2 template
        TEMPLATE_FILE="make.conf.j2"

        # Extract keys (hostnames) from the YAML file
        # The following command gets the top-level keys in the YAML file
        KEYS=$(yq e '. | keys | .[]' "${CONFIG_FILE}")

        # Iterate over each key and process it
        for key in ${KEYS}; do
            if [[ "${key}" != "null" && ! -z "${key}" ]]; then
                # Generate temporary YAML file for the current key
                TEMP_YAML_FILE="temp_${key}.yml"
                yq e ".${key}" "${CONFIG_FILE}" > "${TEMP_YAML_FILE}"

                # Define the output file path
                OUTPUT_FILE="files/${key}/etc/portage/make.conf"

                # Ensure the output directory exists
                mkdir -p "$(dirname "${OUTPUT_FILE}")"

                # Run j2 using the temporary YAML file
                ~/.venv/bin/jinja2 --format=yaml "${TEMPLATE_FILE}" "${TEMP_YAML_FILE}" > "${OUTPUT_FILE}"

                echo "Generated configuration for ${key} in ${OUTPUT_FILE}"

                # Clean up the temporary file
                rm -f "${TEMP_YAML_FILE}"
            fi
        done

    - name: Prepare chroot
      run: |
        mount -t proc /proc ${{ env.stage4_fs }}/proc
        mount --rbind /dev ${{ env.stage4_fs }}/dev
        mount -t devpts devpts ${{ env.stage4_fs }}/dev/pts
        mount -t tmpfs tmpfs ${{ env.stage4_fs }}/dev/shm
        mount -t tmpfs tmpfs ${{ env.stage4_fs }}/var/tmp
        mount -t tmpfs tmpfs ${{ env.stage4_fs }}/var/cache

        rsync -vrtza files/common/ ${{ env.stage4_fs }}/
        rsync -vrtza files/${{ env.target }}/ ${{ env.stage4_fs }}/

        tree ${{ env.stage4_fs }}/etc/portage/

        cp step2.sh ${{ env.stage4_fs }}/
        yq eval ".[\"${{ env.target }}\"]" config.yml > ${{ env.stage4_fs }}/config.yml

        bash setup_packages.sh ${{ env.stage4_fs }}

    - name: synchronize remote bucket
      env:
        AWS_BUCKET: pierre-packages
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        if aws s3 sync --delete s3://${AWS_BUCKET}/stage4/${target}/binpkgs/ /dl/binpkgs/${{ env.target }}/ | tee >(grep -v '^Completed' > /dev/tty); then
            echo "AWS sync succeeded"
        else
            echo "AWS sync failed"
        fi
        mkdir -p ${{ env.stage4_fs }}/var/cache/binpkgs
        mount -o bind /dl/binpkgs/${{ env.target }}/ ${{ env.stage4_fs }}/var/cache/binpkgs

    - name: chroot
      run: |
        cp chroot/00.sh ${{ env.stage4_fs }}/00.sh
        chroot ${{ env.stage4_fs }} /bin/bash /00.sh ${{ env.target }}
        chroot ${{ env.stage4_fs }} /usr/bin/eclean packages

    - name: push back changes
      env:
        AWS_BUCKET: pierre-packages
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        # push back changes (useful to remove outdated packages right away)
        if aws s3 sync --delete  /dl/binpkgs/${{ env.target }}/ s3://${AWS_BUCKET}/stage4/${target}/binpkgs/ | tee >(grep -v '^Completed' > /dev/tty); then
            echo "AWS sync succeeded"
        else
            echo "AWS sync failed"
        fi

    - name: build kernel
      run: |
        cp chroot/01_kernel.sh ${{ env.stage4_fs }}/01_kernel.sh
        chroot ${{ env.stage4_fs }} /bin/bash /01_kernel.sh ${{ env.target }}

    - name: push kernel archive to s3
      env:
        AWS_BUCKET: pierre-packages
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        aws s3 cp ${{ env.stage4_fs }}/kernel-*.tgz s3://${AWS_BUCKET}/stage4/${target}/

    - name: quirks
      run: |
        cp chroot/02_quirks.sh ${{ env.stage4_fs }}/02_quirks.sh
        chroot ${{ env.stage4_fs }} /bin/bash /02_quirks.sh ${{ env.target }}

    - name: rebuild world
      run: |
        cp chroot/03_rebuild_world.sh ${{ env.stage4_fs }}/03_rebuild_world.sh
        chroot ${{ env.stage4_fs }} /bin/bash /03_rebuild_world.sh ${{ env.target }}

    - name: push back changes
      env:
        AWS_BUCKET: pierre-packages
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        if aws s3 sync --delete  /dl/binpkgs/${{ env.target }}/ s3://${AWS_BUCKET}/stage4/${target}/binpkgs/ | tee >(grep -v '^Completed' > /dev/tty); then
            echo "AWS sync succeeded"
        else
            echo "AWS sync failed"
        fi

    - name: configure stage4
      run: |
        cp chroot/04_configure.sh ${{ env.stage4_fs }}/04_configure.sh
        chroot ${{ env.stage4_fs }} /bin/bash /04_configure.sh ${{ env.target }}

    - name: build browsers if needed
      if: false  # Skipping browsers for this build
      run: |
        cp chroot/05_browsers.sh ${{ env.stage4_fs }}/05_browsers.sh
        bash ${{ env.stage4_fs }}/05_browsers.sh ${{ env.stage4_fs }}

    - name: push back binpkgs
      env:
        AWS_BUCKET: pierre-packages
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        if aws s3 sync --delete /dl/binpkgs/${{ env.target }}/ s3://${AWS_BUCKET}/stage4/${target}/binpkgs/ | tee >(grep -v '^Completed' > /dev/tty); then
            echo "AWS sync succeeded"
        else
            echo "AWS sync failed"
        fi

    - name: push back portage conf
      env:
        AWS_BUCKET: pierre-packages
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        if aws s3 sync --delete ${{ env.stage4_fs }}/etc/portage/ s3://${AWS_BUCKET}/stage4/${target}/portage/ | tee >(grep -v '^Completed' > /dev/tty); then
            echo "AWS sync succeeded"
        else
            echo "AWS sync failed"
        fi

    - name: umount chroot
      env:
        AWS_BUCKET: pierre-packages
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      if: always()
      run: |

        # Base directory for search
        search_dir="${{ env.stage4_fs }}/var/tmp/portage"

        # Find files and process them
        find "$search_dir" -type f -name "*build.log" | while read -r file; do
            # Extract category and package from path
            category=$(echo "$file" | awk -F'/' '{print $(NF-3)}')
            package=$(echo "$file" | awk -F'/' '{print $(NF-2)}')

            # New file name format: <category>_<package>.log
            new_file_name="${category}_${package}.log"

            # Directory where the file is located
            dir=$(dirname "$file")

            # Full path for the new file
            new_file_path="$dir/$new_file_name"

            # Rename the file
            mv "$file" "$new_file_path"

            # Upload to S3
            aws s3 cp "$new_file_path" "s3://${AWS_BUCKET}/stage4/${target}/$new_file_name"
        done


        sync
        for mount in ${{ env.stage4_fs }}/dev/pts ${{ env.stage4_fs }}/dev/shm ${{ env.stage4_fs }}/proc ${{ env.stage4_fs }}/dev ${{ env.stage4_fs }}/var/tmp ${{ env.stage4_fs }}/var/cache/binpkgs ${{ env.stage4_fs }}/var/cache; do
          if mountpoint -q "$mount"; then
            echo "Attempting lazy unmount of $mount"
            umount -l "$mount" || echo "Failed to lazily unmount $mount."
          else
            echo "$mount is not a mount-point or already unmounted."
          fi
        done

    - name: archive
      env:
        AWS_BUCKET: pierre-packages
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        tar cfz /${target}-stage4.tgz -C ${{ env.stage4_fs }} .
        md5sum /${target}-stage4.tgz > /${target}-stage4.tgz.md5
        cat /${target}-stage4.tgz.md5
        aws s3 cp /${target}-stage4.tgz s3://${AWS_BUCKET}/stage4/${target}/${target}-stage4.tgz
        aws s3 cp /${target}-stage4.tgz.md5 s3://${AWS_BUCKET}/stage4/${target}/${target}-stage4.tgz.md5
